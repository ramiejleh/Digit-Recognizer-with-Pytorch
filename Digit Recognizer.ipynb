{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer with Pytorch\n",
    "A simple approach with decent results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from collections import OrderedDict\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "Load the data by reading from the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data ready for training\n",
    "Split the training data into input data and labels. Then normalize the input data by dividing by 255 and it is ready to be converted to a tensor. \n",
    "Then split the training data into training and validation data and create the dataloaders. And of course load and normalize the test data and convert it to a tensor as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(df_train.drop(['label'], axis=1).values.astype('float32')) / 255\n",
    "labels = torch.tensor(df_train['label'].values.astype(np.float32)).long()\n",
    "test_data = torch.tensor(df_test.values.astype('float32')) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting dataloaders ready for training\n",
    "train_tensor_dataset = torch.utils.data.TensorDataset(train_data, labels)\n",
    "\n",
    "#Splitting the dataset into train and validate datasets\n",
    "train_size = int(0.8 * len(train_tensor_dataset))\n",
    "validate_size = len(train_tensor_dataset) - train_size\n",
    "train_dataset, validate_dataset = torch.utils.data.random_split(train_tensor_dataset, [train_size, validate_size])\n",
    "\n",
    "dataloaders = OrderedDict([\n",
    "    ('train', torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)),\n",
    "    ('validate', torch.utils.data.DataLoader(validate_dataset, batch_size=64, shuffle=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "Define a function that creates the model with input size, hidden layers, output size and dropout probability as parameters. \n",
    "The model has 2 hidden layers with ReLU as an activation function for the hidden layers and LogSoftmax for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_size, hidden_layer=[4096, 2048], output_size=10, drop_p=0.5):\n",
    "    model = nn.Sequential(OrderedDict([('layer1', nn.Linear(input_size, hidden_layer[0])),\n",
    "                                            ('ReLU1', nn.ReLU()),\n",
    "                                            ('layer2', nn.Linear(hidden_layer[0], hidden_layer[1])),\n",
    "                                            ('ReLU2', nn.ReLU()),\n",
    "                                            ('layer3', nn.Linear(hidden_layer[1], output_size)),\n",
    "                                            ('dropout', nn.Dropout(p=drop_p)),\n",
    "                                            ('output', nn.LogSoftmax(dim=-1))]))\n",
    "    return model\n",
    "model = create_model(train_data.shape[1], [200,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation\n",
    "Define a function to validate the model with the validation chunk of the training data which was split earlier. \n",
    "The function takes the model, dataloader, device(cpu, cuda) and the criterion as parameters. \n",
    "The function returns the loss and the accuracy of the model with the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, dataloader, device, criterion):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    model.to(device)\n",
    "    model.float()\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model.forward(inputs)\n",
    "        test_loss += criterion(outputs, labels).item() / len(dataloader)\n",
    "        ps = torch.exp(outputs)\n",
    "        _, predicted = torch.max(ps.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "Define a function to train the model with the training data. \n",
    "The function takes the model, dataloader, learning rate, device(cpu, cuda) and epochs(number of iterations) as parameters. \n",
    "The function prints the loss and accuracy of the model with the training data and uses the validate_model function to print the loss and accuracy of the validation data with each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3..  Training Loss: 1.546..  Training Accuracy: 44 % Test Loss: 0.596..  Test Accuracy: 87 %\n",
      "Epoch: 1/3..  Training Loss: 0.397..  Training Accuracy: 67 % Test Loss: 0.324..  Test Accuracy: 90 %\n",
      "Epoch: 1/3..  Training Loss: 0.297..  Training Accuracy: 75 % Test Loss: 0.261..  Test Accuracy: 92 %\n",
      "Epoch: 1/3..  Training Loss: 0.247..  Training Accuracy: 79 % Test Loss: 0.226..  Test Accuracy: 93 %\n",
      "Epoch: 1/3..  Training Loss: 0.220..  Training Accuracy: 82 % Test Loss: 0.208..  Test Accuracy: 93 %\n",
      "Finished Epoch!\n",
      "Epoch: 2/3..  Training Loss: 0.768..  Training Accuracy: 57 % Test Loss: 0.229..  Test Accuracy: 93 %\n",
      "Epoch: 2/3..  Training Loss: 0.172..  Training Accuracy: 78 % Test Loss: 0.170..  Test Accuracy: 94 %\n",
      "Epoch: 2/3..  Training Loss: 0.160..  Training Accuracy: 84 % Test Loss: 0.159..  Test Accuracy: 95 %\n",
      "Epoch: 2/3..  Training Loss: 0.149..  Training Accuracy: 87 % Test Loss: 0.163..  Test Accuracy: 95 %\n",
      "Epoch: 2/3..  Training Loss: 0.134..  Training Accuracy: 89 % Test Loss: 0.146..  Test Accuracy: 95 %\n",
      "Finished Epoch!\n",
      "Epoch: 3/3..  Training Loss: 0.491..  Training Accuracy: 56 % Test Loss: 0.163..  Test Accuracy: 95 %\n",
      "Epoch: 3/3..  Training Loss: 0.110..  Training Accuracy: 83 % Test Loss: 0.133..  Test Accuracy: 95 %\n",
      "Epoch: 3/3..  Training Loss: 0.106..  Training Accuracy: 88 % Test Loss: 0.129..  Test Accuracy: 96 %\n",
      "Epoch: 3/3..  Training Loss: 0.086..  Training Accuracy: 91 % Test Loss: 0.121..  Test Accuracy: 96 %\n",
      "Epoch: 3/3..  Training Loss: 0.100..  Training Accuracy: 92 % Test Loss: 0.132..  Test Accuracy: 95 %\n",
      "Finished Epoch!\n",
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "def train_network(model, dataloader, learning_rate=0.001, device='cuda', epochs=3):\n",
    "    print_every = 100\n",
    "    steps = 0\n",
    "    model.to(device)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for ii, (inputs, labels) in enumerate(dataloader):\n",
    "            steps += 1\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(inputs)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            # accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    test_loss, accuracy = validate_model(model, dataloaders['validate'], device, criterion)\n",
    "\n",
    "                print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                      \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n",
    "                      \"Training Accuracy: %d %%\" % (100 * correct / total),\n",
    "                      \"Test Loss: {:.3f}.. \".format(test_loss),\n",
    "                      \"Test Accuracy: %d %%\" % (accuracy))\n",
    "\n",
    "                running_loss = 0\n",
    "        print('Finished Epoch!')\n",
    "    print('Finished Training!')\n",
    "train_network(model, dataloaders['train'], 0.001, 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results as a CSV\n",
    "Run the model with the test data and get the top result and add it to the csv next to the imageId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for inputs in test_data:\n",
    "    with torch.no_grad():\n",
    "        output = model.forward(torch.tensor(inputs))\n",
    "        ps = torch.exp(output)\n",
    "        results = np.append(results, ps.topk(1)[1].numpy()[0])\n",
    "results = results.astype(int)\n",
    "index = [x+1 for x in df_test.index.tolist()]\n",
    "df = pd.DataFrame({'ImageId': index, 'Label':results})\n",
    "df.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
